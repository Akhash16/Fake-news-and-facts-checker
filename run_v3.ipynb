{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "from urllib.request import urlopen,Request\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import validators\n",
    "from newsapi import NewsApiClient\n",
    "from googleapiclient.discovery import build\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pickle', 'rb') as mod:\n",
    "                    model = pickle.load(mod)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Working!\"\n",
    "\n",
    "@app.route(\"/sms\", methods=['POST'])\n",
    "def sms_reply():\n",
    "    \"\"\"Respond to incoming calls with a simple text message.\"\"\"\n",
    "    # Fetch the message\n",
    "    msg = request.form.get('Body')\n",
    "\n",
    "\n",
    "    resp = MessagingResponse()\n",
    "    # Create reply\n",
    "    \n",
    "\n",
    "    #resp.message(\"Title is: {}\".format(title))\n",
    "    if(msg == \"hi\" or msg == 'Hi'):\n",
    "        resp.message(\"FAKE NEWS DETECTOR AND FACTS CHECKER:\\n1.Article checker \\n 2.Facts checker\")\n",
    "    elif(msg == '1'):\n",
    "        resp.message(\"Enter the URL of the article\")\n",
    "    elif(msg == '2'):\n",
    "        resp.message(\"Enter the fact or a short sentence\")\n",
    "    #resp.message(\"You said: {}\".format(msg))\n",
    "    else:\n",
    "        valid=validators.url(msg)\n",
    "        if valid==True:\n",
    "            my_url = msg\n",
    "            req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            uClient = urlopen(req)\n",
    "            page_html = uClient.read()\n",
    "            uClient.close()\n",
    "            page_soup = soup(page_html,'html.parser')\n",
    "            title = page_soup.h1.text\n",
    "\n",
    "            newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "            news_sources = newsapi.get_sources()\n",
    "            top_headlines = newsapi.get_top_headlines(q = title, language = 'en',)\n",
    "            all_articles = newsapi.get_everything(q = title, language = 'en',)\n",
    "            \n",
    "            source2 = []\n",
    "            description2 = []\n",
    "            \n",
    "            for article in all_articles['articles']:\n",
    "                    source2.append(article['source']['name'])\n",
    "                    description2.append(article['description'])\n",
    "            \n",
    "            if(top_headlines['articles'] == []  and all_articles['articles'] == []):\n",
    "                pred = model.predict([title])\n",
    "                ans = \"The news is mostly \" + pred[0]\n",
    "                resp.message(ans)\n",
    "            else:\n",
    "                resp.message('The news is REAL')\n",
    "                resp.message(source2[0])\n",
    "                resp.message(description2[0])\n",
    "\n",
    "\n",
    "        else:\n",
    "            list_of_stopwords = list(stopwords.words('english'))\n",
    "            tokenized_text = word_tokenize(msg)\n",
    "            clean_msg = ''\n",
    "            for word in tokenized_text:\n",
    "                word = word.lower()\n",
    "                if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "                    clean_msg += word + ' '\n",
    "            \n",
    "            API_KEY='AIzaSyBEbc15F1s35_bgvC8eupXt0MpGkV92PnA'\n",
    "            SERVICE=build(\"factchecktools\",\"v1alpha1\",developerKey=API_KEY)\n",
    "            userQuery=clean_msg\n",
    "            \n",
    "            request1=SERVICE.claims().search(query=userQuery)\n",
    "            response=request1.execute()\n",
    "\n",
    "\n",
    "            if not bool(response):\n",
    "                \n",
    "                source1 = []\n",
    "                description1 = []\n",
    "                newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "                news_sources = newsapi.get_sources()\n",
    "                query = msg\n",
    "                tokenized_text1 = word_tokenize(query)\n",
    "                word_count = len(tokenized_text1)\n",
    "                clean_msg1 = ''\n",
    "                count=0\n",
    "                for word in tokenized_text1:\n",
    "                        word = word.lower()\n",
    "                        if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "                            clean_msg1 += word + ' '\n",
    "                all_articles = newsapi.get_everything(q = clean_msg1,sort_by = 'relevancy', language = 'en',)\n",
    "                \n",
    "                if(all_articles['articles'] not = []):\n",
    "                \n",
    "                    for article in all_articles['articles']:\n",
    "                        source1.append(article['source']['name'])\n",
    "                        description1.append(article['description'])\n",
    "\n",
    "\n",
    "                    for i in range(len(source1)):\n",
    "                        for j in range(word_count):\n",
    "                            if (tokenized_text[j] in description1[i]):\n",
    "                                count = count+1;\n",
    "                        if (count >= word_count/2):\n",
    "                            newsapi_source = source1[i]\n",
    "                            newsapi_description = description1[i]\n",
    "                            break\n",
    "                \n",
    "\n",
    "                if((all_articles['articles'] == []) or (count < word_count/2)):\n",
    "                    #resp.message(\"The news is FAKE\")\n",
    "                    user_text = msg\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#                     text = input()\n",
    "#                     words = ['what ', 'who ', 'why ', 'how ', 'Is ']\n",
    "#                     questions = []\n",
    "#                     for word in words:\n",
    "#                         if word == 'Is ' and text.find('is')>0:\n",
    "#                             word_list = text.split()\n",
    "#                             new = ' '.join([i for i in word_list if i not in 'is'])\n",
    "#                             questions.append('Is ' + new + ' ?')\n",
    "#                         else:\n",
    "#                             questions.append(word + text + ' ?')\n",
    "                    \n",
    "                    temp = user_text.split()\n",
    "                    word_count1 = len(temp)\n",
    "                    processed_text = '+'.join(temp)\n",
    "\n",
    "\n",
    "                    my_url = f'https://www.google.com/search?q={processed_text}' \n",
    "                    req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    uClient = urlopen(req)\n",
    "                    page_html = uClient.read()\n",
    "                    uClient.close()\n",
    "                    page_soup = soup(page_html,'html.parser')\n",
    "\n",
    "\n",
    "                    container = page_soup.find_all('div', {'class': 'BNeawe s3v9rd AP7Wnd'})\n",
    "                    \n",
    "                    list_of_stopwords = list(stopwords.words('english'))\n",
    "                    tokenized_user_text = word_tokenize(user_text)\n",
    "                    clean_user_text = []\n",
    "                    for word in tokenized_user_text:\n",
    "                            word = word.lower()\n",
    "                            if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "                                clean_user_text.append(word)\n",
    "\n",
    "                    for i in range(len(container)):\n",
    "                        tokenized_result = word_tokenize(container[i].text)\n",
    "                        clean_result = ''\n",
    "                        clean_searches = []\n",
    "                        for word in tokenized_user_text:\n",
    "                                word = word.lower()\n",
    "                                if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "                                    clean_result = word + ' '\n",
    "                                    clean_searches.append(clean_result)\n",
    "\n",
    "                    count1 = 0\n",
    "                    for i in range(len(clean_searches)):\n",
    "                        for j in range(len(clean_user_text)):\n",
    "                            if (clean_user_text[j] in clean_searches[i]):\n",
    "                                count1 = count1+1;\n",
    "                        if (count1 >= word_count1/2):\n",
    "                            resp.message('The news is REAL')\n",
    "                            resp.message(container[0].text)  \n",
    "                            break\n",
    "                    if(count1 < word_count1/2):\n",
    "                        resp.message('The news is FAKE')\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    resp.message('The news is REAL')\n",
    "                    resp.message(newsapi_source)\n",
    "                    resp.message(newsapi_description)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                result  = response['claims'][0]['claimReview'][0]['textualRating']\n",
    "                website = response['claims'][0]['claimReview'][0]['publisher']['name']\n",
    "                url = response['claims'][0]['claimReview'][0]['url']\n",
    "                resp.message(result)\n",
    "                resp.message(website)\n",
    "                resp.message(url) \n",
    "\n",
    "            \n",
    "\n",
    "    return str(resp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
