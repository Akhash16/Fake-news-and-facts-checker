{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import validators\n",
    "from newsapi import NewsApiClient\n",
    "from googleapiclient.discovery import build\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pickle', 'rb') as mod:\n",
    "    model = pickle.load(mod)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Working!\"\n",
    "\n",
    "\n",
    "@app.route(\"/sms\", methods=['POST'])\n",
    "def sms_reply():\n",
    "    \"\"\"Respond to incoming calls with a simple text message.\"\"\"\n",
    "    # Fetch the message\n",
    "    msg = request.form.get('Body')\n",
    "\n",
    "    resp = MessagingResponse()\n",
    "    # Create reply\n",
    "\n",
    "    # resp.message(\"Title is: {}\".format(title))\n",
    "    if (msg == \" \" or msg == '0'):\n",
    "        resp.message('''Hi Hello ! \\n This is Jazzy - A News Detection Bot \\n I am capable of doing some Tasks! \\n Type a Number to navigate: \\n 1.Article checking \\n 2.Article Summarization \\n 3.Facts checking \\n 4.Url Expander \\n Share me \\n https://wa.me/+14155238886?text=\"join came-poor\" ''')\n",
    "\n",
    "    if(msg=='1'):\n",
    "        resp.message(\"Enter the Url of the article\")\n",
    "        valid = validators.url(msg)\n",
    "        if valid == True:\n",
    "            my_url = msg\n",
    "            req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            uClient = urlopen(req)\n",
    "            page_html = uClient.read()\n",
    "            uClient.close()\n",
    "            page_soup = soup(page_html, 'html.parser')\n",
    "            title = page_soup.h1.text\n",
    "\n",
    "            newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "            news_sources = newsapi.get_sources()\n",
    "            top_headlines = newsapi.get_top_headlines(q=title, language='en', )\n",
    "            all_articles = newsapi.get_everything(q=title, language='en', )\n",
    "\n",
    "            source2 = []\n",
    "            description2 = []\n",
    "            if(top_headlines['articles'] != [] and all_articles['articles'] != []):\n",
    "                for article in all_articles['articles']:\n",
    "                    source2.append(article['source']['name'])\n",
    "                    description2.append(article['description'])\n",
    "                resp.message('The news is REAL')\n",
    "                resp.message(source2[0])\n",
    "                resp.message(description2[0])\n",
    "            else:\n",
    "                pred = model.predict([title])\n",
    "                ans = \"The news is mostly \" + pred[0]\n",
    "                resp.message(ans)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"The Url does not exists !\")\n",
    "            \n",
    "    if(msg==2):\n",
    "        pass\n",
    "            \n",
    "    if(msg==3):\n",
    "        resp.message('''Type a word or short sentence (in English) related to the fact or news you want to check, and weâ€™ll send you the related results.\\n ðŸ‘€ Example: If youâ€™ve read a news about anything, type the news headlines or a short sentence like: does iphone 12 series are packed without chargers?''')\n",
    "        list_of_stopwords = list(stopwords.words('english'))\n",
    "        tokenized_text = word_tokenize(msg)\n",
    "        clean_msg = ''\n",
    "        for word in tokenized_text:\n",
    "            word = word.lower()\n",
    "            if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "                clean_msg += word + ' '\n",
    "\n",
    "        API_KEY = 'AIzaSyBEbc15F1s35_bgvC8eupXt0MpGkV92PnA'\n",
    "        SERVICE = build(\"factchecktools\", \"v1alpha1\", developerKey=API_KEY)\n",
    "        userQuery = clean_msg\n",
    "        request1 = SERVICE.claims().search(query=userQuery)\n",
    "        response = request1.execute()\n",
    "\n",
    "        result = response['claims'][0]['claimReview'][0]['textualRating']\n",
    "        website = response['claims'][0]['claimReview'][0]['publisher']['name']\n",
    "        url = response['claims'][0]['claimReview'][0]['url']\n",
    "        resp.message(result)\n",
    "        resp.message(website)\n",
    "        resp.message(url)\n",
    "\n",
    "        if not bool(response):\n",
    "\n",
    "            source1 = []\n",
    "            description1 = []\n",
    "            newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "            news_sources = newsapi.get_sources()\n",
    "            query = msg\n",
    "            tokenized_text1 = word_tokenize(query)\n",
    "            word_count = len(tokenized_text1)\n",
    "            clean_msg1 = ''\n",
    "            count = 0\n",
    "            for word in tokenized_text1:\n",
    "                word = word.lower()\n",
    "                if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "                    clean_msg1 += word + ' '\n",
    "            all_articles = newsapi.get_everything(q=clean_msg1, sort_by='relevancy', language='en', )\n",
    "\n",
    "            if (all_articles['articles'] !=[]):\n",
    "                for article in all_articles['articles']:\n",
    "                    source1.append(article['source']['name'])\n",
    "                    description1.append(article['description'])\n",
    "                for i in range(len(source1)):\n",
    "                    for j in range(word_count):\n",
    "                        if (tokenized_text[j] in description1[i]):\n",
    "                            count = count + 1;\n",
    "                            if (count >= word_count / 2):\n",
    "                                newsapi_source = source1[i]\n",
    "                                newsapi_description = description1[i]\n",
    "                                break\n",
    "\n",
    "                resp.message('The news is REAL')\n",
    "                resp.message(newsapi_source)\n",
    "                resp.message(newsapi_description)\n",
    "    \n",
    "    if(msg==4):\n",
    "        pass\n",
    "#                     text = input()\n",
    "        #                     words = ['what ', 'who ', 'why ', 'how ', 'Is ']\n",
    "        #                     questions = []\n",
    "        #                     for word in words:\n",
    "        #                         if word == 'Is ' and text.find('is')>0:\n",
    "        #                             word_list = text.split()\n",
    "        #                             new = ' '.join([i for i in word_list if i not in 'is'])\n",
    "        #                             questions.append('Is ' + new + ' ?')\n",
    "        #                         else:\n",
    "        #                             questions.append(word + text + ' ?')\n",
    "\n",
    "    temp = msg.split()\n",
    "    word_count1 = len(temp)\n",
    "    processed_text = '+'.join(temp)\n",
    "\n",
    "    my_url = f'https://www.google.com/search?q={processed_text}'\n",
    "    req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    uClient = urlopen(req)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html, 'html.parser')\n",
    "\n",
    "    container = page_soup.find_all('div', {'class': 'BNeawe s3v9rd AP7Wnd'})\n",
    "\n",
    "    list_of_stopwords = list(stopwords.words('english'))\n",
    "    tokenized_user_text = word_tokenize(user_text)\n",
    "    clean_user_text = []\n",
    "    for word in tokenized_user_text:\n",
    "        word = word.lower()\n",
    "        if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "            clean_user_text.append(word)\n",
    "\n",
    "    for i in range(len(container)):\n",
    "        tokenized_result = word_tokenize(container[i].text)\n",
    "        clean_result = ''\n",
    "        clean_searches = []\n",
    "        for word in tokenized_user_text:\n",
    "            word = word.lower()\n",
    "            if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "                clean_result = word + ' '\n",
    "                clean_searches.append(clean_result)\n",
    "\n",
    "    count1 = 0\n",
    "    for i in range(len(clean_searches)):\n",
    "        for j in range(len(clean_user_text)):\n",
    "            if (clean_user_text[j] in clean_searches[i]):\n",
    "                count1 = count1 + 1;\n",
    "            if (count1 >= word_count1 / 2):\n",
    "                resp.message(container[0].text)\n",
    "\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        resp.message(\"press 0 to main menu\")\n",
    "\n",
    "    return str(resp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
