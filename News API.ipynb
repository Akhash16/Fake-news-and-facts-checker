{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news is REAL\n",
      "The Times of India\n",
      "When images of 'dinosaur eggs' found in Tamil Nadu's Perambalur district became viral on the internet, it seemed like a great discovery. But it proved to be a false dawn. As per a TOI report, experts have determined that these 'eggs' are actually ammonite sed…\n"
     ]
    }
   ],
   "source": [
    "#without wordcount \n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "source1 = []\n",
    "description1 = []\n",
    "newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "news_sources = newsapi.get_sources()\n",
    "\n",
    "sources_list = ['ABC News','BBC News','Wired','ABC News (AU)','BBC Sport','Bloomberg','Business Insider','Business Insider (UK)','Buzzfeed','CBC News','CBS News','CNN','ESPN','Fox News','Google News','Google News (India)','Google News (UK)','Google News (Australia)','IGN','The Times of India','The Wall Street Journal','The Washington Times','TechCrunch','The Hindu']\n",
    "\n",
    "query = 'Dinosaur Eggs Found in Tamil Nadu Perambalur'\n",
    "\n",
    "all_articles = newsapi.get_everything(q = query,sort_by= 'relevancy', language = 'en')\n",
    "for article in all_articles['articles']:\n",
    "    source1.append(article['source']['name'])\n",
    "    description1.append(article['description'])\n",
    "\n",
    "check =  any(item in source1 for item in sources_list)    \n",
    "    \n",
    "if((all_articles['articles'] == []) or (check == False)):\n",
    "    print(\"The news is FAKE\")\n",
    "if(check == True):\n",
    "    print('The news is REAL')\n",
    "    print(source1[0])\n",
    "    print(description1[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Times of India\n",
      "When images of 'dinosaur eggs' found in Tamil Nadu's Perambalur district became viral on the internet, it seemed like a great discovery. But it proved to be a false dawn. As per a TOI report, experts have determined that these 'eggs' are actually ammonite sed…\n"
     ]
    }
   ],
   "source": [
    "#with wordcount\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "source1 = []\n",
    "description1 = []\n",
    "sources_list = ['ABC News','BBC News','Wired','ABC News (AU)','BBC Sport','Bloomberg','Business Insider','Business Insider (UK)','Buzzfeed','CBC News','CBS News','CNN','ESPN','Fox News','Google News','Google News (India)','Google News (UK)','Google News (Australia)','IGN','The Times of India','The Wall Street Journal','The Washington Times','TechCrunch','The Hindu']\n",
    "newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "news_sources = newsapi.get_sources()\n",
    "\n",
    "query = 'Dinosaur Eggs Found in Tamil Nadu Perambalur'\n",
    "\n",
    "tokenized_text1 = word_tokenize(query)\n",
    "word_count = len(tokenized_text1)\n",
    "clean_msg1 = ''\n",
    "count=0\n",
    "list_of_stopwords = list(stopwords.words('english'))\n",
    "for word in tokenized_text1:\n",
    "        word = word.lower()\n",
    "        if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "            clean_msg1 += word + ' '\n",
    "all_articles = newsapi.get_everything(q = clean_msg1,sort_by = 'relevancy', language = 'en',)\n",
    "\n",
    "if(all_articles['articles']  != []):\n",
    "    for article in all_articles['articles']:\n",
    "        source1.append(article['source']['name'])\n",
    "        description1.append(article['description'])\n",
    "        \n",
    "check =  any(item in source1 for item in sources_list)\n",
    "\n",
    "if((all_articles['articles'] == []) or (check == False)):\n",
    "    print(\"The news is FAKE\")\n",
    "if(check == True):    \n",
    "    for i in range(len(source1)):\n",
    "        for j in range(word_count):\n",
    "            if (tokenized_text1[j] in description1[i]):\n",
    "                count = count+1;\n",
    "        if (count >= word_count/2):\n",
    "            newsapi_source = source1[i]\n",
    "            newsapi_description = description1[i]\n",
    "            \n",
    "            print(newsapi_source)\n",
    "            print(newsapi_description)\n",
    "            break\n",
    "    if (count < word_count/2):\n",
    "        print('no info')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
