{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ML_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REAL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'SPB dude died due to health issues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stopwords = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(msg)\n",
    "clean_msg = ''\n",
    "for word in tokenized_text:\n",
    "        word = word.lower()\n",
    "        if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "            clean_msg += word + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spb dude died due health issues '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            list_of_stopwords = list(stopwords.words('english'))\n",
    "            tokenized_text = word_tokenize(msg)\n",
    "            clean_msg = ''\n",
    "            for word in tokenized_text:\n",
    "                word = word.lower()\n",
    "                if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "                    clean_msg += word + ' '\n",
    "                    \n",
    "                    \n",
    "            #news api part        \n",
    "            source1 = []\n",
    "            description1 = []\n",
    "            newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "            news_sources = newsapi.get_sources()\n",
    "            all_articles = newsapi.get_everything(q = clean_msg, language = 'en',)\n",
    "            for article in all_articles['articles']:\n",
    "                source1.append(article['source']['name'])\n",
    "                description1.append(article['description'])\n",
    "\n",
    "\n",
    "            if(all_articles['articles'] == []):\n",
    "\n",
    "                API_KEY='AIzaSyBEbc15F1s35_bgvC8eupXt0MpGkV92PnA'\n",
    "                SERVICE=build(\"factchecktools\",\"v1alpha1\",developerKey=API_KEY)\n",
    "                userQuery=clean_msg\n",
    "\n",
    "                request1=SERVICE.claims().search(query=userQuery)\n",
    "                response=request1.execute()\n",
    "                \n",
    "                if bool(response):\n",
    "                    result  = response['claims'][0]['claimReview'][0]['textualRating']\n",
    "                    website = response['claims'][0]['claimReview'][0]['publisher']['name']\n",
    "                    url = response['claims'][0]['claimReview'][0]['url']\n",
    "                    resp.message(result)\n",
    "                    resp.message(website)\n",
    "                    resp.message(url)\n",
    "                    \n",
    "                else:\n",
    "                    resp.message(\"The news is FAKE\")\n",
    "                \n",
    "            else:\n",
    "                resp.message('The news is REAL')\n",
    "                resp.message(source1[0])\n",
    "                resp.message(description1[0])   \n",
    "                    \n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "            else:\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION 1\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "source1 = []\n",
    "description1 = []\n",
    "newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "news_sources = newsapi.get_sources()\n",
    "query ='Sardar IPS passed out'\n",
    "all_articles = newsapi.get_everything(q = query,sort_by = 'relevancy', language = 'en',)\n",
    "for article in all_articles['articles']:\n",
    "    source1.append(article['source']['name'])\n",
    "    description1.append(article['description'])\n",
    "if(all_articles['articles'] == []):\n",
    "    print('no info')\n",
    "else:\n",
    "    for i in range(len(source1)):\n",
    "        if(description1[i].find(query) == -1):\n",
    "            continue\n",
    "        else:\n",
    "            print('The news is REAL')\n",
    "            print(source1[i])\n",
    "            print(description1[i])\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news is REAL\n",
      "Wired\n",
      "On a “body farm,” researchers are exploring whether the nutrients from human cadavers can change the look of plants, which authorities might use to locate missing persons.\n"
     ]
    }
   ],
   "source": [
    "#VERSION 2\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "source1 = []\n",
    "description1 = []\n",
    "newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "news_sources = newsapi.get_sources()\n",
    "query ='human body'\n",
    "all_articles = newsapi.get_everything(q = query,sort_by = 'relevancy', language = 'en',)\n",
    "for article in all_articles['articles']:\n",
    "    source1.append(article['source']['name'])\n",
    "    description1.append(article['description'])\n",
    "if(all_articles['articles'] == []):\n",
    "    print('no info')\n",
    "else:\n",
    "    print('The news is REAL')\n",
    "    print(source1[0])\n",
    "    print(description1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hindu\n",
      "Noted playback singer S.P. Balasubrahmanyam, who died of prolonged illness in Chennai on Friday, had a special attachment with Vizianagaram that produced great playback singers such as P. Susheela.His\n"
     ]
    }
   ],
   "source": [
    "#VERSION 3\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from newsapi import NewsApiClient\n",
    "source1 = []\n",
    "description1 = []\n",
    "newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "news_sources = newsapi.get_sources()\n",
    "query ='SPB died'\n",
    "list_of_stopwords = list(stopwords.words('english'))\n",
    "tokenized_text = word_tokenize(query)\n",
    "clean_msg = ''\n",
    "for word in tokenized_text:\n",
    "        word = word.lower()\n",
    "        if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "            clean_msg += word + ' '\n",
    "all_articles = newsapi.get_everything(q = clean_msg,sort_by = 'relevancy', language = 'en',)\n",
    "for article in all_articles['articles']:\n",
    "    source1.append(article['source']['name'])\n",
    "    description1.append(article['description'])\n",
    "    \n",
    "\n",
    "word_count = len(tokenized_text)\n",
    "\n",
    "count=0\n",
    "    \n",
    "if(all_articles['articles'] == []):\n",
    "    print('no info')\n",
    "else:\n",
    "    for i in range(len(source1)):\n",
    "        for j in range(word_count):\n",
    "            if (tokenized_text[j] in description1[i]):\n",
    "                count = count+1;\n",
    "        if (count >= word_count/2):\n",
    "            print(source1[i])\n",
    "            print(description1[i])\n",
    "            break         \n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
