{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "from urllib.request import urlopen,Request\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import validators\n",
    "from newsapi import NewsApiClient\n",
    "from googleapiclient.discovery import build\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pickle', 'rb') as mod:\n",
    "                    model = pickle.load(mod)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Working!\"\n",
    "\n",
    "@app.route(\"/sms\", methods=['POST'])\n",
    "def sms_reply():\n",
    "    \"\"\"Respond to incoming calls with a simple text message.\"\"\"\n",
    "    # Fetch the message\n",
    "    msg = request.form.get('Body')\n",
    "\n",
    "\n",
    "    resp = MessagingResponse()\n",
    "    # Create reply\n",
    "    \n",
    "\n",
    "    #resp.message(\"Title is: {}\".format(title))\n",
    "    if(msg == \"hi\" or msg == 'Hi'):\n",
    "        resp.message(\"FAKE NEWS DETECTOR AND FACTS CHECKER:\\n1.Article checker \\n 2.Facts checker\")\n",
    "    elif(msg == '1'):\n",
    "        resp.message(\"Enter the URL of the article\")\n",
    "    elif(msg == '2'):\n",
    "        resp.message(\"Enter the fact or a short sentence\")\n",
    "    #resp.message(\"You said: {}\".format(msg))\n",
    "    else:\n",
    "        valid=validators.url(msg)\n",
    "        if valid==True:\n",
    "            my_url = msg\n",
    "            req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            uClient = urlopen(req)\n",
    "            page_html = uClient.read()\n",
    "            uClient.close()\n",
    "            page_soup = soup(page_html,'html.parser')\n",
    "            title = page_soup.h1.text\n",
    "\n",
    "            newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "            news_sources = newsapi.get_sources()\n",
    "            top_headlines = newsapi.get_top_headlines(q = title, language = 'en',)\n",
    "            all_articles = newsapi.get_everything(q = title, language = 'en',)\n",
    "            \n",
    "            source2 = []\n",
    "            description2 = []\n",
    "            \n",
    "            for article in all_articles['articles']:\n",
    "                    source2.append(article['source']['name'])\n",
    "                    description2.append(article['description'])\n",
    "            \n",
    "            if(top_headlines['articles'] == []  and all_articles['articles'] == []):\n",
    "                pred = model.predict([title])\n",
    "                ans = \"The news is mostly \" + pred[0]\n",
    "                resp.message(ans)\n",
    "            else:\n",
    "                resp.message('The news is REAL')\n",
    "                resp.message(source2[0])\n",
    "                resp.message(description2[0])\n",
    "\n",
    "\n",
    "        else:\n",
    "            list_of_stopwords = list(stopwords.words('english'))\n",
    "            tokenized_text = word_tokenize(msg)\n",
    "            clean_msg = ''\n",
    "            for word in tokenized_text:\n",
    "                word = word.lower()\n",
    "                if not word in list_of_stopwords and word != '.' and word != \"''\" and word!=\"``\"and word !=']' and word !='!' and word !='%' and word !='&' and word !='?' and word !='//' and word !=';' and word !='|' and word != ' ' and word != \"'\" and word !='\"' and  word !='[' and word != '@' and word != ',' and word !='#' and word !='..' and word !='-' and word !='(' and word !=')' and word != '...' and word != '/' and word !=':':\n",
    "                    clean_msg += word + ' '\n",
    "            \n",
    "            API_KEY='AIzaSyBEbc15F1s35_bgvC8eupXt0MpGkV92PnA'\n",
    "            SERVICE=build(\"factchecktools\",\"v1alpha1\",developerKey=API_KEY)\n",
    "            userQuery=clean_msg\n",
    "            \n",
    "            request1=SERVICE.claims().search(query=userQuery)\n",
    "            response=request1.execute()\n",
    "\n",
    "\n",
    "            if not bool(response):\n",
    "                \n",
    "                source1 = []\n",
    "                description1 = []\n",
    "                newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "                news_sources = newsapi.get_sources()\n",
    "\n",
    "                all_articles = newsapi.get_everything(q = clean_msg, language = 'en',)\n",
    "                for article in all_articles['articles']:\n",
    "                    source1.append(article['source']['name'])\n",
    "                    description1.append(article['description'])\n",
    "                \n",
    "\n",
    "                if(all_articles['articles'] == []):\n",
    "                    #resp.message(\"The news is FAKE\")\n",
    "                    user_text = input(\"Enter any sentence: \")\n",
    "                    temp = user_text.split()\n",
    "                    processed_text = '+'.join(temp)\n",
    "\n",
    "\n",
    "                    my_url = f'https://www.google.com/search?q={processed_text}' \n",
    "                    req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    uClient = urlopen(req)\n",
    "                    page_html = uClient.read()\n",
    "                    uClient.close()\n",
    "                    page_soup = soup(page_html,'html.parser')\n",
    "\n",
    "\n",
    "                    container = page_soup.find_all('div', {'class': 'BNeawe s3v9rd AP7Wnd'})\n",
    "                    content = container[0].text\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    resp.message('The news is REAL')\n",
    "                    resp.message(source1[0])\n",
    "                    resp.message(description1[0])\n",
    "                \n",
    "\n",
    "            else:\n",
    "                result  = response['claims'][0]['claimReview'][0]['textualRating']\n",
    "                website = response['claims'][0]['claimReview'][0]['publisher']['name']\n",
    "                url = response['claims'][0]['claimReview'][0]['url']\n",
    "                resp.message(result)\n",
    "                resp.message(website)\n",
    "                resp.message(url) \n",
    "\n",
    "            \n",
    "\n",
    "    return str(resp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen,Request\n",
    "from bs4 import BeautifulSoup as soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any sentence: barcelona won the ucl final 2020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The club won the 1996 final 1–0 against Rapid Wien, before losing 1–0 against Barcelona while attempting to defend their title in 1997. PSG also featured in the\\u200b\\xa0...\\n\\nVenue: Estádio da Luz, Lisbon\\nDate: 23 August 2020\\nMan of the Match: Kingsley Coman (Bayern Munich)\\nReferee: Daniele Orsato (Italy)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text = input(\"Enter any sentence: \")\n",
    "temp = user_text.split()\n",
    "processed_text = '+'.join(temp)\n",
    "\n",
    "\n",
    "my_url = f'https://www.google.com/search?q={processed_text}' \n",
    "req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "uClient = urlopen(req)\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "page_soup = soup(page_html,'html.parser')\n",
    "\n",
    "\n",
    "container1 = page_soup.find_all('div', {'class': 'BNeawe s3v9rd AP7Wnd'})\n",
    "container1[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
