{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import validators\n",
    "from newsapi import NewsApiClient\n",
    "from googleapiclient.discovery import build\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pickle', 'rb') as mod:\n",
    "    model = pickle.load(mod)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Online\"\n",
    "\n",
    "@app.route('/sms', methods=['POST'])\n",
    "def bot():\n",
    "    incoming_msg = request.values.get('Body', '')\n",
    "    \n",
    "    resp = MessagingResponse()\n",
    "    msg = resp.message()\n",
    "    responded = False\n",
    "    \n",
    "    if 'Menu' in incoming_msg or 'menu' in incoming_msg:\n",
    "        text = 'Hi Hello ! \\n This is Jazzy - A News Detection Bot \\n I am capable of doing some Tasks! \\n Type a Number to navigate: \\n 1.Article checking \\n 2.Article Summarization \\n 3.Facts checking \\n 4.Url Expander \\n Share me \\n https://wa.me/+14155238886?text=\"join came-poor\" '\n",
    "        msg.body(text)\n",
    "        responded = True\n",
    "        \n",
    "    if 'Commands' in incoming_msg or 'commands' in incoming_msg:\n",
    "        text = 'List of commands:'\n",
    "        msg.body(text)\n",
    "        responded = True\n",
    "        \n",
    "    if '/AC ' in incoming_msg:\n",
    "        \n",
    "        msg1 = incoming_msg[4:]\n",
    "        valid = validators.url(msg1)\n",
    "        if valid == True:\n",
    "            my_url = msg1\n",
    "            req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            uClient = urlopen(req)\n",
    "            page_html = uClient.read()\n",
    "            uClient.close()\n",
    "            page_soup = soup(page_html, 'html.parser')\n",
    "            title = page_soup.h1.text\n",
    "\n",
    "            newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "            news_sources = newsapi.get_sources()\n",
    "            top_headlines = newsapi.get_top_headlines(q=title, language='en', )\n",
    "            all_articles = newsapi.get_everything(q=title, language='en', )\n",
    "\n",
    "            source2 = []\n",
    "            description2 = []\n",
    "            if(top_headlines['articles'] != [] and all_articles['articles'] != []):\n",
    "                for article in all_articles['articles']:\n",
    "                    source2.append(article['source']['name'])\n",
    "                    description2.append(article['description'])\n",
    "                msg.body('The news is REAL')\n",
    "                msg.body(source2[0])\n",
    "                msg.body(description2[0])\n",
    "            else:\n",
    "                pred = model.predict([title])\n",
    "                ans = \"The news is mostly \" + pred[0]\n",
    "                msg.body(ans)\n",
    "\n",
    "\n",
    "        else:\n",
    "            msg.body(\"The Url does not exists !\")\n",
    "        \n",
    "        responded = True\n",
    "    \n",
    "    if '/AS' in incoming_msg:\n",
    "        msg2 = incoming_msg[4:]\n",
    "        my_url = msg2\n",
    "        req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        try:\n",
    "            uClient = urlopen(req)\n",
    "        except :\n",
    "            uClient = urlopen(my_url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup = soup(page_html,'html.parser')\n",
    "\n",
    "        article_text =' '.join(map(lambda p: p.text,page_soup.find_all('p')))\n",
    "\n",
    "        from summa.summarizer import summarize\n",
    "        ans = summarize(article_text,ratio=0.2)\n",
    "        msg.body(ans)\n",
    "        responded = True  \n",
    "        \n",
    "    if '/FC' in incoming_msg:\n",
    "        msg3 = incoming_msg[4:]\n",
    "        list_of_stopwords = list(stopwords.words('english'))\n",
    "        tokenized_text = word_tokenize(msg3)\n",
    "        clean_msg = ''\n",
    "        for word in tokenized_text:\n",
    "            word = word.lower()\n",
    "            if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "                clean_msg += word + ' '\n",
    "\n",
    "        API_KEY = 'AIzaSyBEbc15F1s35_bgvC8eupXt0MpGkV92PnA'\n",
    "        SERVICE = build(\"factchecktools\", \"v1alpha1\", developerKey=API_KEY)\n",
    "        userQuery = clean_msg\n",
    "        request1 = SERVICE.claims().search(query=userQuery)\n",
    "        response = request1.execute()\n",
    "\n",
    "        result = response['claims'][0]['claimReview'][0]['textualRating']\n",
    "        website = response['claims'][0]['claimReview'][0]['publisher']['name']\n",
    "        url = response['claims'][0]['claimReview'][0]['url']\n",
    "        msg.body(result)\n",
    "        msg.body(website)\n",
    "        msg.body(url)\n",
    "\n",
    "        if not bool(response):\n",
    "\n",
    "            source1 = []\n",
    "            description1 = []\n",
    "            newsapi = NewsApiClient(api_key='cc8998f479954041b5f845f0b4491050')\n",
    "            news_sources = newsapi.get_sources()\n",
    "            query = msg3\n",
    "            tokenized_text1 = word_tokenize(query)\n",
    "            word_count = len(tokenized_text1)\n",
    "            clean_msg1 = ''\n",
    "            count = 0\n",
    "            for word in tokenized_text1:\n",
    "                word = word.lower()\n",
    "                if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "                    clean_msg1 += word + ' '\n",
    "            all_articles = newsapi.get_everything(q=clean_msg1, sort_by='relevancy', language='en', )\n",
    "\n",
    "            if (all_articles['articles'] !=[]):\n",
    "                for article in all_articles['articles']:\n",
    "                    source1.append(article['source']['name'])\n",
    "                    description1.append(article['description'])\n",
    "                for i in range(len(source1)):\n",
    "                    for j in range(word_count):\n",
    "                        if (tokenized_text[j] in description1[i]):\n",
    "                            count = count + 1;\n",
    "                            if (count >= word_count / 2):\n",
    "                                msg.body('The news is REAL')\n",
    "                                msg.body(source1[i])\n",
    "                                msg.body(description1[i])\n",
    "                                break\n",
    "        \n",
    "        temp = msg3.split()\n",
    "        word_count1 = len(temp)\n",
    "        processed_text = '+'.join(temp)\n",
    "\n",
    "        my_url = f'https://www.google.com/search?q={processed_text}'\n",
    "        req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        uClient = urlopen(req)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup = soup(page_html, 'html.parser')\n",
    "\n",
    "        container = page_soup.find_all('div', {'class': 'BNeawe s3v9rd AP7Wnd'})\n",
    "\n",
    "        list_of_stopwords = list(stopwords.words('english'))\n",
    "        tokenized_user_text = word_tokenize(msg3)\n",
    "        clean_user_text = []\n",
    "        for word in tokenized_user_text:\n",
    "            word = word.lower()\n",
    "            if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "                clean_user_text.append(word)\n",
    "\n",
    "        for i in range(len(container)):\n",
    "            tokenized_result = word_tokenize(container[i].text)\n",
    "            clean_result = ''\n",
    "            clean_searches = []\n",
    "            for word in tokenized_user_text:\n",
    "                word = word.lower()\n",
    "                if not word in list_of_stopwords and word != '.' and word != \"''\" and word != \"``\" and word != ']' and word != '!' and word != '%' and word != '&' and word != '?' and word != '//' and word != ';' and word != '|' and word != ' ' and word != \"'\" and word != '\"' and word != '[' and word != '@' and word != ',' and word != '#' and word != '..' and word != '-' and word != '(' and word != ')' and word != '...' and word != '/' and word != ':':\n",
    "                    clean_result = word + ' '\n",
    "                    clean_searches.append(clean_result)\n",
    "\n",
    "        count1 = 0\n",
    "        for i in range(len(clean_searches)):\n",
    "            for j in range(len(clean_user_text)):\n",
    "                if (clean_user_text[j] in clean_searches[i]):\n",
    "                    count1 = count1 + 1;\n",
    "                if (count1 >= word_count1 / 2):\n",
    "                    msg.body(container[0].text)\n",
    "                    break\n",
    "    \n",
    "        responded = True\n",
    "            \n",
    "    if '/UE' in incoming_msg:\n",
    "        msg4 = incoming_msg[4:]\n",
    "        shortened_url = msg4\n",
    "        my_url = f'https://checkshorturl.com/expand.php?u={shortened_url}' \n",
    "        req = Request(my_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        uClient = urlopen(req)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup = soup(page_html,'html.parser')\n",
    "\n",
    "        content = page_soup.find_all('tr')\n",
    "        long_url = content[0].text\n",
    "        title = content[6].text\n",
    "        description = content[7].text\n",
    "        msg.body(long_url)\n",
    "        msg.body(title)\n",
    "        msg.body(description)\n",
    "        \n",
    "        responded = True\n",
    "\n",
    "\n",
    "    if responded == False:\n",
    "        msg.body('no commands found')\n",
    "\n",
    "    return str(resp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
